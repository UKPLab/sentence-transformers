{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenceTransformers ONNX Inference Model\n",
    "\n",
    "This notebook is intended to give a brief introduction on how to create ONNX models based on a given SentenceTransformers model. This tutorial is only applicable for models, which have been released on the transformers model hub: https://huggingface.co/sentence-transformers\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "Initially, we start with the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf bert-base-nli-stsb-mean-tokens\n",
    "import time\n",
    "import pprint\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "import onnx\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "\n",
    "from termcolor import colored\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import convert_graph_to_onnx\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pprint = pp.pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to provide some performance measurments, this notebook has been run on the following hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mGPU available True\u001b[0m\n",
      "\u001b[32mGPU Name: Tesla V100-SXM2-32GB\u001b[0m\n",
      "\u001b[32mGPU Count: 1\u001b[0m\n",
      "\u001b[32mCORE Count: 48\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colored(f\"GPU available {torch.cuda.is_available()}\", \"green\"))\n",
    "print(colored(f\"GPU Name: {torch.cuda.get_device_name(0)}\", \"green\"))\n",
    "print(colored(f\"GPU Count: {torch.cuda.device_count()}\", \"green\"))\n",
    "print(colored(f\"CORE Count: {multiprocessing.cpu_count()}\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a test span and some preliminary information w.r.t. the model to be used.  We also load the raw model from this library as a benchmark and for checking the sanity of our converted ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = \"I am a span. A short span, but nonetheless a span\"\n",
    "\n",
    "model_type = \"bert\"\n",
    "model_name = f\"{model_type}-base-nli-stsb-mean-tokens\"\n",
    "model_access = f\"sentence-transformers/{model_name}\"\n",
    "\n",
    "model_raw = SentenceTransformer(model_name, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pipeline\n",
    "\n",
    "We subequently load the FeatureExtractionPipeline from the transformers library. This step is ultimately not necessary if you know all the input shapes and config of the model you want to use. However, using the corresponding functions from convert_graph_to_onnx significantly eases creating our custom model.\n",
    "\n",
    "The resulting variables will be used for the torch export call later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = transformers.FeatureExtractionPipeline(\n",
    "    model=transformers.AutoModel.from_pretrained(model_access),\n",
    "    tokenizer=transformers.AutoTokenizer.from_pretrained(model_access, use_fast=True),\n",
    "    framework=\"pt\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "config = model_pipeline.model.config\n",
    "tokenizer = model_pipeline.tokenizer\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_names, output_names, dynamic_axes, tokens = convert_graph_to_onnx.infer_shapes(\n",
    "        model_pipeline, \n",
    "        \"pt\"\n",
    "    )\n",
    "    ordered_input_names, model_args = convert_graph_to_onnx.ensure_valid_input(\n",
    "        model_pipeline.model, tokens, input_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n",
      "['output_0', 'output_1']\n",
      "{   'attention_mask': {0: 'batch', 1: 'sequence'},\n",
      "    'input_ids': {0: 'batch', 1: 'sequence'},\n",
      "    'output_0': {0: 'batch', 1: 'sequence'},\n",
      "    'output_1': {0: 'batch'},\n",
      "    'token_type_ids': {0: 'batch', 1: 'sequence'}}\n",
      "{   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]),\n",
      "    'input_ids': tensor([[ 101, 2023, 2003, 1037, 7099, 6434,  102]]),\n",
      "    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]])}\n",
      "['input_ids', 'attention_mask', 'token_type_ids']\n",
      "(   tensor([[ 101, 2023, 2003, 1037, 7099, 6434,  102]]),\n",
      "    tensor([[1, 1, 1, 1, 1, 1, 1]]),\n",
      "    tensor([[0, 0, 0, 0, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "pprint(input_names)\n",
    "pprint(output_names)\n",
    "pprint(dynamic_axes)\n",
    "pprint(tokens)\n",
    "pprint(ordered_input_names)\n",
    "pprint(model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our application, we want to create a custom transformers model with a different output than the feature extraction pipeline.\n",
    "We only use the output of the feature extractor, but return the pooled sentence embedding.\n",
    "\n",
    "We must add a new output for the pooled sentence embedding. This output is of fixed size, as opposed to (for example) the original output_0, corresponding to the token embeddings.\n",
    "\n",
    "All other (input) variables can be left unchanged, because they are identical for the models.\n",
    "\n",
    "Therefore, we change variables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence_embedding']\n",
      "{   'attention_mask': {0: 'batch', 1: 'sequence'},\n",
      "    'input_ids': {0: 'batch', 1: 'sequence'},\n",
      "    'sentence_embedding': {0: 'batch'},\n",
      "    'token_type_ids': {0: 'batch', 1: 'sequence'}}\n"
     ]
    }
   ],
   "source": [
    "del dynamic_axes[\"output_0\"] # Delete unused output\n",
    "del dynamic_axes[\"output_1\"] # Delete unused output\n",
    "\n",
    "output_names = [\"sentence_embedding\"]\n",
    "dynamic_axes[\"sentence_embedding\"] = {0: 'batch'}\n",
    "\n",
    "# Check that everything worked\n",
    "pprint(output_names)\n",
    "pprint(dynamic_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the SentenceTransformer Model\n",
    "\n",
    "Next, we create the custom transformers model, which is based on the BertModel of the original model contained in the pipeline (make sure to get the **inheritance** right.)\n",
    "\n",
    "If you want to add further outputs, you will have to modify the dynamic_axes and output_names accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(transformers.BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Naming alias for ONNX output specification\n",
    "        # Makes it easier to identify the layer\n",
    "        self.sentence_embedding = torch.nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        # Get the token embeddings from the base model\n",
    "        token_embeddings = super().forward(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )[0]\n",
    "        # Stack the pooling layer on top of it\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return self.sentence_embedding(sum_embeddings / sum_mask)\n",
    "\n",
    "# Create the new model based on the config of the original pipeline\n",
    "model = SentenceTransformer(config=config).from_pretrained(model_access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that both, the original model and the newly created model, result in the same output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    model_raw.encode(span),\n",
    "    model(**tokenizer(span, return_tensors=\"pt\")).squeeze().detach().numpy(),\n",
    "    atol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Model to ONNX\n",
    "\n",
    "The following step is heavily based on the original [convert_graph_to_onnx.py](https://github.com/huggingface/transformers/blob/master/src/transformers/convert_graph_to_onnx.py) from the transformers library.\n",
    "\n",
    "Important note: The opset version defines the version of the set of operations which can be converted to ONNX. For the given model, the highest opset version is required.\n",
    "\n",
    "Note: The opset version might cause problems with the subsequent call of optimizer.optimize_model from onnxruntime_tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to bert-base-nli-stsb-mean-tokens/bert-base-nli-stsb-mean-tokens.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oborchers/anaconda3/envs/dev/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:192: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, :seq_length]\n",
      "/home/oborchers/anaconda3/envs/dev/lib/python3.8/site-packages/transformers/modeling_utils.py:1672: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert all(\n"
     ]
    }
   ],
   "source": [
    "outdir = Path(model_name)\n",
    "output = outdir / f\"{model_name}.onnx\"\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if output.exists():\n",
    "    print(f\"Model {model_type} exists. Skipping creation\")\n",
    "else:\n",
    "    print(f\"Saving to {output}\")\n",
    "    # This is essentially a copy of transformers.convert_graph_to_onnx.convert\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        model_args,\n",
    "        f=output.as_posix(),\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        do_constant_folding=True,\n",
    "        use_external_data_format=False,\n",
    "        enable_onnx_checker=True,\n",
    "        opset_version=12,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly check if the ONNX model works as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is checked!\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(output)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print('The model is checked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "\n",
    "We are finally able to run an inference session based on the ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = rt.SessionOptions()\n",
    "opt.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "opt.log_severity_level = 3\n",
    "opt.execution_mode = rt.ExecutionMode.ORT_SEQUENTIAL\n",
    "\n",
    "sess = rt.InferenceSession(str(output), opt) # Loads the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything else, lets validate if the outputs of the ONNX model correspond to the raw models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer.encode_plus(span)\n",
    "model_input = {name : np.atleast_2d(value) for name, value in model_input.items()}\n",
    "onnx_result = sess.run(None, model_input)\n",
    "\n",
    "assert np.allclose(model_raw.encode(span), onnx_result, atol=1e-6)\n",
    "assert np.allclose(\n",
    "    model(**tokenizer(span, return_tensors=\"pt\")).squeeze().detach().numpy(), \n",
    "    onnx_result, \n",
    "    atol=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are able to run the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Encoding Benchmark\n",
    "\n",
    "This benchmark simulates encoding spans on the fly without any batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.9 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 200 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 200\n",
    "model_raw.encode(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21 ms ± 56.6 µs per loop (mean ± std. dev. of 7 runs, 200 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 200\n",
    "model_input = tokenizer.encode_plus(span)\n",
    "model_input = {name : np.atleast_2d(value) for name, value in model_input.items()}\n",
    "output = sess.run(None, model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this example we are able to speed up the online inference speed of bert-base-nli-stsb-mean-tokens by a factor of *6-7* on an empty V100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Encoding Benchmark\n",
    "\n",
    "Lets benchmark the model in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_spans = 100_000\n",
    "batch_size = 32\n",
    "sentences = [\"I am a very short span.\" for _ in range(no_spans)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentences, batch_size=batch_size):\n",
    "    \"\"\"Wrapped by method for line profiler\"\"\"\n",
    "    iterator = range(0, len(sentences), batch_size)\n",
    "    for start_index in iterator:\n",
    "        sentences_batch = sentences[start_index:start_index+batch_size]\n",
    "\n",
    "        tokens = tokenizer(sentences_batch)\n",
    "        tokens = {name: np.atleast_2d(value) for name, value in tokens.items()}\n",
    "        out = sess.run(None, tokens)[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "_ = model_raw.encode(\n",
    "    sentences=sentences,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990.25\n"
     ]
    }
   ],
   "source": [
    "sentences_per_second = 1 / ((end-start) / no_spans) \n",
    "print(f\"{sentences_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "convert(sentences)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3994.79\n"
     ]
    }
   ],
   "source": [
    "sentences_per_second = 1 / ((end-start) / no_spans) \n",
    "print(f\"{sentences_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the batched case, the ONNX model achieves a 2x speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf bert-base-nli-stsb-mean-tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
